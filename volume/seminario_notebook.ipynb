{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶úüîó‚Äã Descubriendo el Poder de ‚ÄãLangchain: \n",
    "Transformaci√≥n de Texto en Datos Estructurados‚Äã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# üêõ Debug ON/OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òÄÔ∏è ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåë OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# üìú Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "import warnings\n",
    "\n",
    "# import warning menssages that start with \"UserWarning:\"\n",
    "warnings.filterwarnings(\"always\", category=UserWarning, module='transformers')\n",
    "\n",
    "class ExtractJsonOutputParser(StrOutputParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def parse_result(self, output):\n",
    "        result = super().parse_result(output)\n",
    "        try:\n",
    "            # make a regular expression to find the content between the <json> tags\n",
    "            json_regex = re.compile(r'<json>(.*)</json>', re.DOTALL)\n",
    "            # find the content between the <json> tags\n",
    "            json_content = json_regex.findall(result)[0]\n",
    "            return json_content\n",
    "        except:\n",
    "            return result\n",
    "        \n",
    "def llama_prompt(system, human, **kwargs):\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=f\"<<SYS>>\\n{system}\\n<</SYS>>\\n\\n[INST]\\n{human}\\n[/INST]\",\n",
    "        partial_variables=kwargs\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert on the Harry Potter universe. Use your bast knowledge of the Harry Potter universe to perform the tasks the user demands. If you don't have the necessary information to perform a task, indicate so, NEVER MADE THINGS UP.\n",
    "\"\"\"\n",
    "\n",
    "json_format = \"\"\"\n",
    "<json>\n",
    "{\n",
    "    'highlights': array of\n",
    "    {\n",
    "        'order': int,\n",
    "        'location': string,\n",
    "        'summary': string,\n",
    "    }\n",
    "}\n",
    "</json>\n",
    "\"\"\"\n",
    "\n",
    "def count_tokens(text: str, model) -> int:\n",
    "    return len(model.pipeline.tokenizer.encode(text))\n",
    "\n",
    "def print_memory(memory, params={}):\n",
    "    print(\"MEMORY BUFFER==>:\")\n",
    "    print(memory.load_memory_variables(params)[\"chat_history\"])\n",
    "    print(\"<==============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ü¶ô Cargar el Modelo: LLama-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sources.llama2_loader import load_llama_llm\n",
    "\n",
    "llm = load_llama_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîó‚Äã Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üí¨ Prompt simple: \n",
    "Resumen de un capitulo de un libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# create prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template=\"Please summarize what happend in the chapter {chapter} of the book {book_title}. Be brief 20 words or less.\"\n",
    ")\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### invoke(): \n",
    "Ejecuta una cadena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"book_title\": \"Harry Potter and the Philosopher's Stone\",\"chapter\": \"1\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch(): \n",
    "Ejecuta la misma cadena varias veces con diferentes entradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.batch([\n",
    "    {\"book_title\": \"Harry Potter and the Philosopher's Stone\",\"chapter\": \"1\"},\n",
    "    {\"book_title\": \"Harry Potter and the Philosopher's Stone\",\"chapter\": \"2\"},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stream():\n",
    "Delvuelve la salida poco a poco, no es necesario que termine de generar todo el prompt (simula que escribe la ia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No funciona\n",
    "for t in chain.stream({\"book_title\": \"Harry Potter and the Philosopher's Stone\",\"chapter\": \"1\"}):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üí¨ü¶ú Prompt con parser:\n",
    "Genera highlights en formato JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí¨Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "system_prompt = \"You are an expert on the Harry Potter universe. Use your bast knowledge of the Harry Potter universe to perform the tasks the user asks. If you don't have the necessary information to perform a task, indicate so, NEVER MADE THINGS UP.\n",
    "\"\"\"\n",
    "\n",
    "human_prompt = \"\"\"\n",
    "Use your knowledge of the Harry Potter universe to list the top 5 highlights that happend in the chapter 1 {chapter} of the book {book_title}. Order the highlights in order of importance. Include the location where the highlight take place. Return the output in a JSON document with the following format, enclose the document with the tags <json> and </json>:\n",
    "{format}\n",
    "\"\"\"\n",
    "\n",
    "json_format = \"\"\"\n",
    "<json>\n",
    "{\n",
    "    'highlights': array of\n",
    "    {\n",
    "        'order': int,\n",
    "        'location': string,\n",
    "        'summary': string,\n",
    "    }\n",
    "}\n",
    "</json>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template=f\"<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n[INST]\\n{human_prompt}\\n[/INST]\",\n",
    "    partial_variables={'format': json_format}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü¶úParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "class ExtractJsonOutputParser(StrOutputParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def parse_result(self, output):\n",
    "        result = super().parse_result(output)\n",
    "        try:\n",
    "            # make a regular expression to find the content between the <json> tags\n",
    "            json_regex = re.compile(r'<json>(.*)</json>', re.DOTALL)\n",
    "            # find the content between the <json> tags\n",
    "            json_content = json_regex.findall(result)[0]\n",
    "            return json_content\n",
    "        except:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "chain = prompt | llm | ExtractJsonOutputParser() | json.loads\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"book_title\": \"Harry Potter and the Philosopher's Stone\",\n",
    "    \"chapter\": \"1\"\n",
    "})\n",
    "\n",
    "print(response.__class__)\n",
    "display(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üîóüîóüîó Encadenando chains:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secuencial\n",
    "Resumen + highlights\n",
    "```\n",
    "üîósummary\n",
    "    |\n",
    "üîóhighlights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "system_prompt = \"You are an expert on the Harry Potter universe. Use your bast knowledge of the Harry Potter universe to perform the tasks the user demands. If you don't have the necessary information to perform a task, indicate so, NEVER MADE THINGS UP.\n",
    "\"\"\"\n",
    "\n",
    "summary_prompt = \"\"\"\n",
    "Use your knowledge of the Harry Potter universe to summarize what happend in the chapter {chapter} of the book {book_title}. Be brief, 50 words maximum.\n",
    "\"\"\"\n",
    "\n",
    "highlights_prompt = \"\"\"\n",
    "Here is the summary of a chapter of a Harry Potter book:\n",
    "{summary}\n",
    "Based in that summary, identify and list the top 5 highlights that happend in it. Never add content that is not included in the summary. Order the highlights in order of importance. Include the location where the highlight take place. Return the output in a JSON document with the following format, enclose the document with the tags <output> and </output>:\n",
    "{format}\n",
    "\"\"\"\n",
    "\n",
    "prompt_summary = llama_prompt(system_prompt, summary_prompt)\n",
    "prompt_highlights = llama_prompt(system_prompt, highlights_prompt, format=json_format)\n",
    "\n",
    "chain_summary = prompt_summary | llm | StrOutputParser()\n",
    "chain_highlights = {\"summary\": chain_summary} | prompt_highlights | llm | ExtractJsonOutputParser()\n",
    "\n",
    "response = chain_highlights.invoke({\n",
    "    \"book_title\": \"Harry Potter and the Philosopher's Stone\",\n",
    "    \"chapter\": \"1\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow\n",
    "Polaridad de una relaci√≥n\n",
    "```\n",
    "      üîóinteractions\n",
    "        /        \\\n",
    "       /          \\\n",
    "  üîópositive   üîónegative\n",
    "       \\           /\n",
    "        \\         /\n",
    "         üîópolarity\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "interactions = (\n",
    "    {'character_1': RunnablePassthrough(), 'character_2': RunnablePassthrough()}\n",
    "    | llama_prompt(system_prompt, \"List five things that happend between the characters {character_1} and {character_2}. Be brief, describe each interaction in 10 words maximum. If you don't have the necessary information to perform a task, indicate so, NEVER MADE THINGS UP.\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | {'interactions': RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "positive = (\n",
    "    llama_prompt(system_prompt, \"Given the next list of interactions between two character, argue if they are positive and why: {interactions}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "negative = (\n",
    "    llama_prompt(system_prompt, \"Given the next list of interactions between two character, argue if they are negative and why: {interactions}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "polarity = (\n",
    "    llama_prompt(system_prompt, \"Your task is to measure the polarity of the relationship between two characters. Assign a value between -1 (very bad relationship) and +1 (very good relationship). Use the next context to help you: \\n {positive} \\n {negative}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    interactions\n",
    "    | {'positive': positive, 'negative': negative}\n",
    "    | polarity\n",
    ")\n",
    "\n",
    "response = chain.invoke((\"Harry Potter\", \"Draco Malfoy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìñ Documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tama√±o de una ventana de contexto de 4000 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventana =\"\"\"\n",
    "xml version='1.0' encoding='utf-8'?\n",
    "\n",
    "CHAPTER  ONE\n",
    "\n",
    "THE BOY WHO LIVED\n",
    "\n",
    "Mr. and Mrs. Dursley, of\n",
    "number four, Privet Drive, were proud to say that they were\n",
    "perfectly normal, thank you very much. They were the last people\n",
    "you‚Äôd expect to be involved in anything strange or\n",
    "mysterious, because they just didn‚Äôt hold with such\n",
    "nonsense.\n",
    "\n",
    "Mr. Dursley was the director of a firm called Grunnings, which\n",
    "made drills. He was a big, beefy man with hardly any neck, although\n",
    "he did have a very large mustache. Mrs. Dursley was thin and blonde\n",
    "and had nearly twice the usual amount of neck, which came in very\n",
    "useful as she spent so much of her time craning over garden fences,\n",
    "spying on the neighbors. The Dursleys had a small son called Dudley\n",
    "and in their opinion there was no finer boy anywhere.\n",
    "\n",
    "The Dursleys had everything they wanted, but they also had a\n",
    "secret, and their greatest fear was that somebody would discover\n",
    "it. They didn‚Äôt think they could bear it if anyone found out\n",
    "about the Potters. Mrs. Potter was Mrs. Dursley‚Äôs sister, but\n",
    "they hadn‚Äôt met for several years; in fact, Mrs. Dursley\n",
    "pretended she didn‚Äôt have a sister, because her sister and\n",
    "her good-for-nothing husband were as unDursleyish as it was\n",
    "possible to be. The Dursleys shuddered to think what the neighbors\n",
    "would say if the Potters arrived in the street. The Dursleys knew\n",
    "that the Potters had a small son, too, but they had never even seen\n",
    "him. This boy was another good reason for keeping the Potters away;\n",
    "they didn‚Äôt want Dudley mixing with a child like that.\n",
    "\n",
    "When Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our\n",
    "story starts, there was nothing about the cloudy sky outside to\n",
    "suggest that strange and mysterious things would soon be happening\n",
    "all over the country. Mr. Dursley hummed as he picked out his most\n",
    "boring tie for work, and Mrs. Dursley gossiped away happily as she\n",
    "wrestled a screaming Dudley into his high chair.\n",
    "\n",
    "None of them noticed a large, tawny owl flutter past the\n",
    "window.\n",
    "\n",
    "At half past eight, Mr. Dursley picked up his briefcase, pecked\n",
    "Mrs. Dursley on the cheek, and tried to kiss Dudley good-bye but\n",
    "missed, because Dudley was now having a tantrum and throwing his\n",
    "cereal at the walls. ‚ÄúLittle tyke,‚Äù chortled Mr.\n",
    "Dursley as he left the house. He got into his car and backed out of\n",
    "number four‚Äôs drive.\n",
    "\n",
    "It was on the corner of the street\n",
    "that he noticed the first sign of something peculiar ‚Äî a cat\n",
    "reading a map. For a second, Mr. Dursley didn‚Äôt realize what\n",
    "he had seen ‚Äî then he jerked his head around to look again.\n",
    "There was a tabby cat standing on the corner of Privet Drive, but\n",
    "there wasn‚Äôt a map in sight. What could he have been thinking\n",
    "of? It must have been a trick of the light. Mr. Dursley blinked and\n",
    "stared at the cat. It stared back. As Mr. Dursley drove around the\n",
    "corner and up the road, he watched the cat in his mirror. It was\n",
    "now reading the sign that said Privet Drive ‚Äî no,\n",
    "looking at the sign; cats couldn‚Äôt read maps or\n",
    "signs. Mr. Dursley gave himself a little shake and put the cat out\n",
    "of his mind. As he drove toward town he thought of nothing except a\n",
    "large order of drills he was hoping to get that day.\n",
    "\n",
    "But on the edge of town, drills were\n",
    "driven out of his mind by something else. As he sat in the usual\n",
    "morning traffic jam, he couldn‚Äôt help noticing that there\n",
    "seemed to be a lot of strangely dressed people about. People in\n",
    "cloaks. Mr. Dursley couldn‚Äôt bear people who dressed in funny\n",
    "clothes ‚Äî the getups you saw on young people! He supposed\n",
    "this was some stupid new fashion. He drummed his fingers on the\n",
    "steering wheel and his eyes fell on a huddle of these weirdos\n",
    "standing quite close by. They were whispering excitedly together.\n",
    "Mr. Dursley was enraged to see that a couple of them weren‚Äôt\n",
    "young at all; why, that man had to be older than he was, and\n",
    "wearing an emerald-green cloak! The nerve of him! But then it\n",
    "struck Mr. Dursley that this was probably some silly stunt ‚Äî\n",
    "these people were obviously collecting for\n",
    "something . . . yes, that would be it. The traffic\n",
    "moved on and a few minutes later, Mr. Dursley arrived in the\n",
    "Grunnings parking lot, his mind back on drills.\n",
    "\n",
    "Mr. Dursley always sat with his back to the window in his office\n",
    "on the ninth floor. If he hadn‚Äôt, he might have found it\n",
    "harder to concentrate on drills that morning. He\n",
    "didn‚Äôt see the owls swooping past in broad daylight, though\n",
    "people down in the street did; they pointed and gazed open-mouthed\n",
    "as owl after owl sped overhead. Most of them had never seen an owl\n",
    "even at nighttime. Mr. Dursley, however, had a perfectly normal,\n",
    "owl-free morning. He yelled at five different people. He made\n",
    "several important telephone calls and shouted a bit more. He was in\n",
    "a very good mood until lunchtime, when he thought he‚Äôd\n",
    "stretch his legs and walk across the road to buy himself a bun from\n",
    "the bakery.\n",
    "\n",
    "He‚Äôd forgotten all about the people in cloaks until he\n",
    "passed a group of them next to the baker‚Äôs. He eyed them\n",
    "angrily as he passed. He didn‚Äôt know why, but they made him\n",
    "uneasy. This bunch were whispering excitedly, too, and he\n",
    "couldn‚Äôt see a single collecting tin. It was on his way back\n",
    "past them, clutching a large doughnut in a bag, that he caught a\n",
    "few words of what they were saying.\n",
    "\n",
    "‚ÄúThe Potters, that‚Äôs right, that‚Äôs what I\n",
    "heard ‚Äî‚Äù\n",
    "\n",
    "‚Äú‚Äî yes, their son, Harry ‚Äî‚Äù\n",
    "\n",
    "Mr. Dursley stopped dead. Fear flooded him. He looked back at\n",
    "the whisperers as if he wanted to say something to them, but\n",
    "thought better of it.\n",
    "\n",
    "He dashed back across the road, hurried up to his office,\n",
    "snapped at his secretary not to disturb him, seized his telephone,\n",
    "and had almost finished dialing his home number when he changed his\n",
    "mind. He put the receiver back down and stroked his mustache,\n",
    "thinking . . . no, he was being stupid. Potter\n",
    "wasn‚Äôt such an unusual name. He was sure there were lots of\n",
    "people called Potter who had a son called Harry. Come to think of\n",
    "it, he wasn‚Äôt even sure his nephew was called Harry.\n",
    "He‚Äôd never even seen the boy. It might have been Harvey. Or\n",
    "Harold. There was no point in worrying Mrs. Dursley; she always got\n",
    "so upset at any mention of her sister. He didn‚Äôt blame her\n",
    "‚Äî if he‚Äôd had a sister like\n",
    "that . . . but all the same, those people in\n",
    "cloaks . . .\n",
    "\n",
    "He found it a lot harder to concentrate on drills that afternoon\n",
    "and when he left the building at five o‚Äôclock, he was still\n",
    "so worried that he walked straight into someone just outside the\n",
    "door.\n",
    "\n",
    "‚ÄúSorry,‚Äù he grunted, as the tiny old man stumbled\n",
    "and almost fell. It was a few seconds before Mr. Dursley realized\n",
    "that the man was wearing a violet cloak. He didn‚Äôt seem at\n",
    "all upset at being almost knocked to the ground. On the contrary,\n",
    "his face split into a wide smile and he said in a squeaky voice\n",
    "that made passersby stare, ‚ÄúDon‚Äôt be sorry, my dear\n",
    "sir, for nothing could upset me today! Rejoice, for You-Know-Who\n",
    "has gone at last! Even Muggles like yourself should be celebrating,\n",
    "this happy, happy day!‚Äù\n",
    "\n",
    "And the old man hugged Mr. Dursley around the middle and walked\n",
    "off.\n",
    "\n",
    "Mr. Dursley stood rooted to the spot. He had been hugged by a\n",
    "complete stranger. He also thought he had been called a Muggle,\n",
    "whatever that was. He was rattled. He hurried to his car and set\n",
    "off for home, hoping he was imagining things, which he had never\n",
    "hoped before, because he didn‚Äôt approve of imagination.\n",
    "\n",
    "As he pulled into the driveway of number four, the first thing\n",
    "he saw ‚Äî and it didn‚Äôt improve his mood ‚Äî was the\n",
    "tabby cat he‚Äôd spotted that morning. It was now sitting on\n",
    "his garden wall. He was sure it was the same one; it had the same\n",
    "markings around its eyes.\n",
    "\n",
    "‚ÄúShoo!‚Äù said Mr. Dursley loudly.\n",
    "\n",
    "The cat didn‚Äôt move. It just gave him a stern look. Was\n",
    "this normal cat behavior? Mr. Dursley wondered. Trying to pull\n",
    "himself together, he let himself into the house. He was still\n",
    "determined not to mention anything to his wife.\n",
    "\n",
    "Mrs. Dursley had had a nice, normal day. She told him over\n",
    "dinner all about Mrs. Next Door‚Äôs problems with her daughter\n",
    "and how Dudley had learned a new word (‚ÄúWon‚Äôt!‚Äù).\n",
    "Mr. Dursley tried to act normally. When Dudley had been put to bed,\n",
    "he went into the living room in time to catch the last report on\n",
    "the evening news:\n",
    "\n",
    "‚ÄúAnd finally, bird-watchers everywhere have reported that\n",
    "the nation‚Äôs owls have been behaving very unusually today.\n",
    "Although owls normally hunt at night and are hardly ever seen in\n",
    "daylight, there have been hundreds of sightings of these birds\n",
    "flying in every direction since sunrise. Experts are unable to\n",
    "explain why the owls have suddenly changed their sleeping\n",
    "pattern.‚Äù The newscaster allowed himself a grin. ‚ÄúMost\n",
    "mysterious. And now, over to Jim McGuffin with the weather. Going\n",
    "to be any more showers of owls tonight, Jim?‚Äù\n",
    "\n",
    "‚ÄúWell, Ted,‚Äù said the weatherman, ‚ÄúI\n",
    "don‚Äôt know about that, but it‚Äôs not only the owls that\n",
    "have been acting oddly today. Viewers as far apart as Kent,\n",
    "Yorkshire, and Dundee have been phoning in to tell me that instead\n",
    "of the rain I promised yesterday, they‚Äôve had a downpour of\n",
    "shooting stars! Perhaps people have been celebrating Bonfire Night\n",
    "early ‚Äî it‚Äôs not until next week, folks! But I can\n",
    "promise a wet night tonight.‚Äù\n",
    "\n",
    "Mr. Dursley sat frozen in his armchair. Shooting stars all over\n",
    "Britain? Owls flying by daylight? Mysterious people in cloaks all\n",
    "over the place? And a whisper, a whisper about the\n",
    "Potters . . .\n",
    "\n",
    "Mrs. Dursley came into the living room carrying two cups of tea.\n",
    "It was no good. He‚Äôd have to say something to her. He cleared\n",
    "his throat nervously. ‚ÄúEr ‚Äî Petunia, dear ‚Äî you\n",
    "haven‚Äôt heard from your sister lately, have you?‚Äù\n",
    "\n",
    "As he had expected, Mrs. Dursley looked shocked and angry. After\n",
    "all, they normally pretended she didn‚Äôt have a sister.\n",
    "\n",
    "‚ÄúNo,‚Äù she said sharply. ‚ÄúWhy?‚Äù\n",
    "\n",
    "‚ÄúFunny stuff on the news,‚Äù Mr. Dursley mumbled.\n",
    "‚ÄúOwls . . . shooting\n",
    "stars . . . and there were a lot of funny-looking\n",
    "people in town today . . .‚Äù\n",
    "\n",
    "‚ÄúSo?‚Äù snapped Mrs. Dursley.\n",
    "\n",
    "‚ÄúWell, I just thought . . .\n",
    "maybe . . . it was something to do\n",
    "with . . . you know . . . her\n",
    "crowd.‚Äù\n",
    "\n",
    "Mrs. Dursley sipped her tea through pursed lips. Mr. Dursley\n",
    "wondered whether he dared tell her he‚Äôd heard the name\n",
    "‚ÄúPotter.‚Äù He decided he didn‚Äôt dare. Instead he\n",
    "said, as casually as he could, ‚ÄúTheir son ‚Äî he‚Äôd\n",
    "be about Dudley‚Äôs age now, wouldn‚Äôt he?‚Äù\n",
    "\n",
    "‚ÄúI suppose so,‚Äù said Mrs. Dursley stiffly.\n",
    "\n",
    "‚ÄúWhat‚Äôs his name again? Howard, isn‚Äôt\n",
    "it?‚Äù\n",
    "\n",
    "‚ÄúHarry. Nasty, common name, if you ask me.‚Äù\n",
    "\n",
    "‚ÄúOh, yes,‚Äù said Mr. Dursley, his heart sinking\n",
    "horribly. ‚ÄúYes, I quite agree.‚Äù\n",
    "\n",
    "He didn‚Äôt say another word on the subject as they went\n",
    "upstairs to bed. While Mrs. Dursley was in the bathroom, Mr.\n",
    "Dursley crept to the bedroom window and peered down into the front\n",
    "garden. The cat was still there. It was staring down Privet Drive\n",
    "as though it were waiting for something.\n",
    "\n",
    "Was he imagining things? Could all this have anything to do with\n",
    "the Potters? If it did . . . if it got out that they\n",
    "were related to a pair of ‚Äî well, he didn‚Äôt think he\n",
    "could bear it.\n",
    "\n",
    "The Dursleys got into bed. Mrs. Dursley fell asleep quickly but\n",
    "Mr. Dursley lay awake, turning it all over in his mind. His last,\n",
    "comforting thought before he fell asleep was that even if the\n",
    "Potters were involved, there was no reason for them to come\n",
    "near him and Mrs. Dursley. The Potters knew very well what he and\n",
    "Petunia thought about them and their kind. . . . He\n",
    "couldn‚Äôt see how he and Petunia could get mixed up in\n",
    "anything that might be going on ‚Äî he yawned and turned over\n",
    "‚Äî it couldn‚Äôt affect\n",
    "them. . . .\n",
    "\n",
    "How very wrong he was.\n",
    "\n",
    "Mr. Dursley might have been drifting into an uneasy sleep, but\n",
    "the cat on the wall outside was showing no sign of sleepiness. It\n",
    "was sitting as still as a statue, its eyes fixed unblinkingly on\n",
    "the far corner of Privet Drive. It didn‚Äôt so much as quiver\n",
    "when a car door slammed on the next street, nor when two owls\n",
    "swooped overhead. In fact, it was nearly midnight before the cat\n",
    "moved at all.\n",
    "\n",
    "A man appeared on the corner the cat\n",
    "had been watching, appeared so suddenly and silently you‚Äôd\n",
    "have thought he‚Äôd just popped out of the ground. The\n",
    "cat‚Äôs tail twitched and its eyes narrowed.\n",
    "\n",
    "Nothing like this man had ever been seen on Privet Drive. He was\n",
    "tall, thin, and very old, judging by the silver of his hair and\n",
    "beard, which were both long enough to tuck into his belt. He was\n",
    "wearing long robes, a purple cloak that swept the ground, and\n",
    "high-heeled, buckled boots. His blue eyes were light, bright, and\n",
    "sparkling behind half-moon spectacles and his nose was very long\n",
    "and crooked, as though it had been broken at least twice. This\n",
    "man‚Äôs name was Albus Dumbledore.\n",
    "\n",
    "Albus Dumbledore didn‚Äôt seem to realize that he had just\n",
    "arrived in a street where everything from his name to his boots was\n",
    "unwelcome. He was busy rummaging in his cloak, looking for\n",
    "something. But he did seem to realize he was being watched, because\n",
    "he looked up suddenly at the cat, which was still staring at him\n",
    "from the other end of the street. For some reason, the sight of the\n",
    "cat seemed to amuse him. He chuckled and muttered, ‚ÄúI should\n",
    "have known.‚Äù\n",
    "\n",
    "He found what he was looking for in his inside pocket. It seemed\n",
    "to be a silver cigarette lighter. He flicked it open, held it up in\n",
    "the air, and clicked it. The nearest street lamp went out with a\n",
    "little pop. He clicked it again ‚Äî the next lamp flickered\n",
    "into darkness. Twelve times he clicked the Put-Outer, until the\n",
    "only lights left on the whole street were two tiny pinpricks in the\n",
    "distance, which were the eyes of the cat watching him. If anyone\n",
    "looked out of their window now, even beady-eyed Mrs. Dursley, they\n",
    "wouldn‚Äôt be able to see anything that was happening down on\n",
    "the pavement. Dumbledore slipped the Put-Outer back inside his\n",
    "cloak and set off down the street toward number four, where he sat\n",
    "down on the wall next to the cat. He didn‚Äôt look at it, but\n",
    "after a moment he spoke to it.\n",
    "\n",
    "‚ÄúFancy seeing you here,\n",
    "Professor McGonagall.‚Äù\n",
    "\n",
    "He turned to smile at the tabby, but it had gone. Instead he was\n",
    "smiling at a rather severe-looking woman who was wearing square\n",
    "glasses exactly the shape of the markings the cat had had around\n",
    "its eyes. She, too, was wearing a cloak, an emerald one. Her black\n",
    "hair was drawn into a tight bun. She looked distinctly r\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üìï Procesar un documento completo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü™ì Dividir documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap  = 200\n",
    ")\n",
    "\n",
    "book_path = \"potter_books/processed/hp_01/chapter_01.md\"\n",
    "chapter = UnstructuredMarkdownLoader(book_path).load()\n",
    "chunks = text_splitter.split_documents(chapter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### üõ†Ô∏è + üñáÔ∏è Procesar y juntar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "result = chain.run(chunks[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### üî¨ Examinar el proceso por dentro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map-Reduce\n",
    "```\n",
    "        üìï{documento}\n",
    "       /            \\\n",
    "  üìú{cacho_0} .. üìú{cacho_N}\n",
    "      |                |\n",
    "üîó{map_chain} .. üîó{map_chain}\n",
    "      \\              /\n",
    "     üîó{reduce_chain}\n",
    "             |\n",
    "     üí¨{final_output}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specialized classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# short summary of each section of the book\n",
    "map_template = \"\"\"\n",
    "The following is a set of paragraph extracted from the chapter {chapter} of the book {book_title}:\n",
    "{docs}\n",
    "Based on this list of docs, please summarize them. Be brief 20, words maximun.\n",
    "\"\"\"\n",
    "map_prompt = llama_prompt(system_prompt, map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt, output_parser=StrOutputParser())\n",
    "\n",
    "# Join several summaries into a single a single bigger summary\n",
    "reduce_template = \"\"\"\n",
    "The following is set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary. Be brief, 250 words maximun.\n",
    "\"\"\"\n",
    "reduce_prompt =  llama_prompt(system_prompt, reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt, output_parser=StrOutputParser())\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    collapse_documents_chain=combine_documents_chain,            \n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    llm_chain=map_chain,\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    document_variable_name=\"docs\",\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "response = map_reduce_chain.run(\n",
    "    input_documents=chunks[0:2], \n",
    "    book_title=\"Harry Potter and the Philosopher's Stone\",\n",
    "    chapter=\"1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable.base import RunnableEach, RunnableSequence\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "map_template = \"\"\"\n",
    "The following is a set of paragraph extracted from the chapter {chapter} of the book {book_title}:\n",
    "{docs}\n",
    "Based on this list of docs, please summarize them. Be brief 20, words maximun.\n",
    "\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = (\n",
    "    {\"doc\": RunnablePassthrough()}\n",
    "    | map_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "reduce_template = \"\"\"\n",
    "The following is set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary. Be brief, 250 words maximun.\n",
    "\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(template=reduce_template)\n",
    "reduce_chain = (\n",
    "    {\"docs\": RunnablePassthrough()}\n",
    "    | reduce_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "map_reduce = (\n",
    "    RunnableEach(bound=map_chain)\n",
    "    | reduce_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Refine\n",
    "```\n",
    "üìï{documento}\n",
    " | \\\n",
    " | üìú{cacho_0}-üîó{initial}-üí¨{partial_output_0} \n",
    " \\                  _________________/\n",
    "  \\                |\n",
    "   üìú{cacho_1}-üîó{refine}-üí¨{partial_output_1} \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Specialized Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, RefineDocumentsChain\n",
    "\n",
    "document_variable_name = \"docs\"\n",
    "initial_response_name = \"prev_response\"\n",
    "\n",
    "# Initial chain\n",
    "initial_prompt = \"\"\"\n",
    "The following is a paragraph extracted from the chapter {chapter} of the book {book_title}:\n",
    "{docs}\n",
    "Based on this paragraph, please summarize them. Be brief 250, words maximun.\n",
    "\"\"\"\n",
    "initial_prompt = llama_prompt(system_prompt, initial_prompt)\n",
    "initial_chain = LLMChain(llm=llm, prompt=initial_prompt, output_parser=StrOutputParser())\n",
    "\n",
    "# Refine chain\n",
    "refine_prompt = \"\"\"\n",
    "Here is the summary of the chapter {chapter} of the book {book_title} done so far:\n",
    "{prev_response}\n",
    "Given the next paragraph, refine the summary. Be brief 250, words maximun.\n",
    "{docs}\n",
    "\"\"\"\n",
    "refine_prompt = llama_prompt(system_prompt, refine_prompt)\n",
    "refine_chain = LLMChain(llm=llm, prompt=refine_prompt, output_parser=StrOutputParser())\n",
    "\n",
    "# how to get the information of a chunk\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"],\n",
    "    template=\"{page_content}\"\n",
    ")\n",
    "\n",
    "# do all the process\n",
    "refine_document_chain = RefineDocumentsChain(\n",
    "    initial_llm_chain=initial_chain,\n",
    "    refine_llm_chain=refine_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name,\n",
    "    initial_response_name=initial_response_name\n",
    ")\n",
    "\n",
    "response = refine_document_chain.run(\n",
    "    input_documents=chunks[0:2], \n",
    "    book_title=\"Harry Potter and the Philosopher's Stone\",\n",
    "    chapter=\"1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable.base import RunnableEach, RunnableSequence, RunnableLambda\n",
    "\n",
    "def create_first_chain():\n",
    "    first_template = \"\"\"\n",
    "    Summarize the next paragraph. Be brief 10 words or less.\\n{doc}\n",
    "    \"\"\"\n",
    "    first_prompt = PromptTemplate.from_template(template=first_template)\n",
    "    first_chain = (\n",
    "        {\"doc\": RunnablePassthrough()}\n",
    "        | first_prompt\n",
    "        | llm\n",
    "        |{'summary': RunnablePassthrough()}\n",
    "    )\n",
    "    return first_chain\n",
    "\n",
    "def create_middle_chain(chunks):\n",
    "    def _create_middle_item_chain(document):\n",
    "        refine_template = \"\"\"\n",
    "        Given the next summary with the information processed so far:\n",
    "        {summary}\n",
    "        Update it with the next paragraph, be brief 10 words or less:\n",
    "        {paragraph}\n",
    "        \"\"\"\n",
    "        middle_prompt = PromptTemplate.from_template(template=refine_template, partial_variables={'paragraph': document.page_content})\n",
    "        return (\n",
    "            middle_prompt\n",
    "            | llm\n",
    "            |{'summary': RunnablePassthrough()}\n",
    "        )\n",
    "        \n",
    "    return [_create_middle_item_chain(d) for d in chunks]\n",
    "    \n",
    "def create_refine_chain(chunks):\n",
    "    chain = RunnableSequence(\n",
    "        first = create_first_chain(),\n",
    "        middle = create_middle_chain(chunks),\n",
    "        last = RunnableLambda(lambda x: x)\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "create_refine_chain(chunks[1:3]).invoke(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìò Interactuar con un documento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü™ì Dividir el documento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from functools import reduce\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap  = 150\n",
    ")\n",
    "\n",
    "book_path = \"potter_books/processed/hp_01\"\n",
    "chapters = [join(book_path, f) for f in listdir(book_path) if isfile(join(book_path, f)) and f.endswith(\".md\")]\n",
    "chapters = reduce(lambda x,y: x + UnstructuredMarkdownLoader(file_path=y).load(), chapters, []) \n",
    "chunks = text_splitter.split_documents(chapters)\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ô∏èüóÑÔ∏è Generar la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ô∏è‚öíÔ∏è Extraer cachos relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "question = \"Who is Harry's best friend?\"\n",
    "\n",
    "docs_chain = RunnablePassthrough() | vectordb.as_retriever(search_kwargs = {'k' : 2})\n",
    "\n",
    "for doc in docs_chain.invoke(question):\n",
    "    print(\"=====================================\")\n",
    "    print(doc.page_content)\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Procesar documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(search_kwargs = {'k' : 2}),\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "question = \"What Hogwarts house is Harry in? Only tell the name of the house\"\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "\n",
    "print(question)\n",
    "print(f\">> {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### üî¨ Examinar el proceso por dentro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replicar proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain_mr = (\n",
    "    {\"context\": RunnablePassthrough() | vectordb.as_retriever(search_kwargs = {'k' : 2}), \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What Hogwarts house is Harry in? Only tell the name of the house\"\n",
    "result = qa_chain_mr.invoke(question)\n",
    "\n",
    "print(question)\n",
    "print(f\">> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examinar prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain_mr = (\n",
    "    {\"context\": RunnablePassthrough() | vectordb.as_retriever(search_kwargs = {'k' : 11}), \"question\": RunnablePassthrough()} \n",
    "    | prompt\n",
    ")\n",
    "\n",
    "qa_prompt_response = qa_chain_mr.invoke(question)\n",
    "print(qa_prompt_response.messages[0].content)\n",
    "print(count_tokens(qa_prompt_response.messages[0].content, llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducir el documento a algo manejable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain, StuffDocumentsChain, ReduceDocumentsChain\n",
    "\n",
    "def make_reduce_chain():\n",
    "\n",
    "    # Join several summaries into a single a single bigger summary\n",
    "    reduce_template = \"\"\"\n",
    "    The following is set of paragraphs from a Harry Potter book:\n",
    "    {docs}\n",
    "    Take these and use your knowledge about the Harry Potter universe to summarize the main points. Be brief, 250 words maximun. Focus on the information that is relevant to the next question:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    reduce_prompt =  llama_prompt(system_prompt, reduce_template)\n",
    "    reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt, output_parser=StrOutputParser())\n",
    "\n",
    "    # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "    combine_documents_chain = StuffDocumentsChain(\n",
    "        llm_chain=reduce_chain, document_variable_name=\"docs\"\n",
    "    )\n",
    "\n",
    "    # Combines and iteravely reduces the mapped documents\n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        collapse_documents_chain=combine_documents_chain,            \n",
    "        token_max=500,\n",
    "    )\n",
    "\n",
    "    reduce_question_chain = (\n",
    "        {\"input_documents\": itemgetter(\"question\") | vectordb.as_retriever(search_kwargs = {'k' : 2}), \"question\": itemgetter(\"question\")}\n",
    "        | reduce_documents_chain\n",
    "        | (lambda x: x['output_text'])\n",
    "    )\n",
    "\n",
    "    return reduce_question_chain\n",
    "\n",
    "# question answer chain\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain_mr = (\n",
    "    {\"context\": make_reduce_chain(), \"question\": itemgetter(\"question\")}\n",
    "    | qa_prompt \n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What Hogwarts house is Harry in? Only tell the name of the house\"\n",
    "result = qa_chain_mr.invoke({'question':question})\n",
    "\n",
    "print(question)\n",
    "print(f\">> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ô∏èüóÑÔ∏è Generar la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap  = 150\n",
    ")\n",
    "\n",
    "book_path = \"potter_books/processed/hp_01\"\n",
    "chapters = [join(book_path, f) for f in listdir(book_path) if isfile(join(book_path, f)) and f.endswith(\".md\")]\n",
    "chapters = reduce(lambda x,y: x + UnstructuredMarkdownLoader(file_path=y).load(), chapters, []) \n",
    "chunks = text_splitter.split_documents(chapters)\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üëéüß†Sin memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(search_kwargs = {'k' : 2}),\n",
    "    chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "question = \"What Hogwarts house is Harry in? Only tell the name of the house\"\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "\n",
    "print(question)\n",
    "print(f\">> {result['result']}\")\n",
    "\n",
    "question = \"Tell me three other people in that house? Only tell their full names\"\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "\n",
    "print(question)\n",
    "print(f\">> {result['result']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üëçüß† Con memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectordb.as_retriever(search_kwargs = {'k' : 2}),\n",
    "    chain_type=\"stuff\",\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "question = \"What Hogwarts house is Harry in? Only tell the name of the house\"\n",
    "\n",
    "print(question)\n",
    "result = conversational_chain({\"question\": question})\n",
    "print(f\">> {result['answer']}\")\n",
    "\n",
    "question = \"Tell me three other people in that house? Only tell their full names\"\n",
    "\n",
    "print(question)\n",
    "result = conversational_chain({\"question\": question})\n",
    "print(f\">> {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## üî¨ Examinar el proceso por dentro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Transformar preguntas:\n",
    "Reformular una pregunta dado el contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def transform_question_chain(memory):\n",
    "    \n",
    "    text_prepare_question = \"\"\"\n",
    "    Given the following conversation and a follow up question rephrase the follow up question to be a standalone question, in its original language. \n",
    "    Be brief, never propose more than one question. Never continue the  conversation\n",
    "    {chat_history}\n",
    "    Follow Up Input: {question}\n",
    "    Standalone Question:\n",
    "    \"\"\"\n",
    "    prompt_prepare_question = llama_prompt(system_prompt, text_prepare_question)\n",
    "\n",
    "    chain_prepare_question = (\n",
    "        {\"chat_history\": lambda x: memory.load_memory_variables({})[\"chat_history\"], \"question\": RunnablePassthrough()}\n",
    "        | prompt_prepare_question\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    return chain_prepare_question\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "memory.save_context(\n",
    "    {'input': 'What Hogwarts house is Harry in? Only tell the name of the house'},\n",
    "    {'output': 'Gryffindor'}\n",
    ")\n",
    "\n",
    "transform_question_chain(memory).invoke(\"Tell me three other people in that house? Only tell their full names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Responder la pregunta modificada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "memory.save_context(\n",
    "    {'input': 'What Hogwarts house is Harry in? Only tell the name of the house'},\n",
    "    {'output': 'Gryffindor'}\n",
    ")\n",
    "\n",
    "text_solve_question = \"\"\"\n",
    "User your knowledge about the Harry Potter universe to answer the following question:\n",
    "QUESTION: {question}\n",
    "USEFULL ANSWER:\n",
    "\"\"\"\n",
    "prompt_solve_question = llama_prompt(system_prompt,text_solve_question)\n",
    "\n",
    "conversational_chain = (\n",
    "    {\"question\": transform_question_chain(memory)}\n",
    "    | prompt_solve_question\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(conversational_chain.invoke(\"Tell me three other people in that house? Be brief, only tell their full names\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Actualizar la memoria cada llamada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "def print_memory(memory, params={}):\n",
    "    print(\"MEMORY BUFFER==>:\")\n",
    "    print(memory.load_memory_variables(params)[\"chat_history\"])\n",
    "    print(\"<==============\")\n",
    "\n",
    "def ask_question(chain, question):\n",
    "    print(f\"Human: {question}\")\n",
    "    print(f\">> AI: {chain.invoke(question)}\")    \n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=False)\n",
    "\n",
    "solve_question_chain = LLMChain(\n",
    "    prompt=prompt_solve_question,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")\n",
    "conversational_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | solve_question_chain\n",
    "    | (lambda x: x['text'])\n",
    ")\n",
    "\n",
    "print_memory(memory)\n",
    "\n",
    "ask_question(conversational_chain, \"What Hogwarts house is Harry in? Only tell the name of the house\")\n",
    "\n",
    "print_memory(memory)\n",
    "\n",
    "ask_question(conversational_chain, \"Tell me three other people in that house? Only tell their full names\")\n",
    "\n",
    "print_memory(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóìÔ∏è Tipos de memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Window Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
    "\n",
    "print_memory(memory)\n",
    "\n",
    "memory.save_context(\n",
    "    {'input': 'What Hogwarts house is Harry in? Only tell the name of the house'},\n",
    "    {'output': 'Gryffindor'}\n",
    ")\n",
    "\n",
    "print_memory(memory)\n",
    "\n",
    "memory.save_context(\n",
    "    {'input': 'Tell me three other people in that house? Only tell their full names'},\n",
    "    {'output': 'Sure! Here are three other people in the house with Harry Potter:\\n1. Ronald Bilius Weasley\\n2. Hermione Jean Granger\\n3. Frederick Gideon Lester'}\n",
    ")\n",
    "\n",
    "print_memory(memory)\n",
    "\n",
    "memory.save_context(\n",
    "    {'input': 'Who is Frederick Gideon Lester?'},\n",
    "    {'output': 'Who knows'}\n",
    ")\n",
    "\n",
    "print_memory(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Conversation Summary Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "def make_summary_buffer():\n",
    "    memory_system_prompt = \"\"\"\n",
    "    You are a helpfull AI in charge of summarizing conversations between a human and an AI model. Please do the summary exactly as the user tells you to do so.\n",
    "    \"\"\"\n",
    "    memory_user_prompt = \"\"\"\n",
    "    Given the next summary of the conversation so far: \n",
    "    {summary}\n",
    "    Use the new lines of conversations to update the summary with new conversation. Be brief, only responde with the new summary, make it shorter than 20 words. Never include information that do not appear in the new lines or the previous summary\n",
    "    {new_lines}\n",
    "    \"\"\"\n",
    "    memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=50, memory_key=\"chat_history\", prompt=llama_prompt(memory_system_prompt, memory_user_prompt))\n",
    "    return memory\n",
    "\n",
    "memory = make_summary_buffer()\n",
    "\n",
    "print_memory(memory)\n",
    "\n",
    "memory.save_context(\n",
    "    {'input': 'What Hogwarts house is Harry in? Only tell the name of the house'},\n",
    "    {'output': 'Gryffindor'}\n",
    ")\n",
    "\n",
    "print_memory(memory)\n",
    "\n",
    "memory.save_context(\n",
    "    {'input': 'Tell me three other people in that house? Only tell their full names'},\n",
    "    {'output': 'Sure! Here are three other people in the house with Harry Potter:\\n1. Ronald Bilius Weasley\\n2. Hermione Jean Granger\\n3. Frederick Gideon Lester'}\n",
    ")\n",
    "\n",
    "print_memory(memory)\n",
    "\n",
    "memory.save_context(\n",
    "    {'input': 'Who is Frederick Gideon Lester? Be brief no more than 20 words'},\n",
    "    {'output': \"Frederick Gideon Lester is a Hogwarts student in Harry's year.\"}\n",
    ")\n",
    "\n",
    "print_memory(memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "embedding_size = 384\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "memory_vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})\n",
    "\n",
    "retriever = memory_vectorstore.as_retriever(search_kwargs=dict(k=2))\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever,  memory_key=\"chat_history\")\n",
    "\n",
    "memory.save_context(\n",
    "    {'input': 'What Hogwarts house is Harry in? Only tell the name of the house'},\n",
    "    {'output': 'Gryffindor'}\n",
    ")\n",
    "memory.save_context(\n",
    "    {'input': 'Tell me three other people in that house? Only tell their full names'},\n",
    "    {'output': 'Sure! Here are three other people in the house with Harry Potter:\\n1. Ronald Bilius Weasley\\n2. Hermione Jean Granger\\n3. Frederick Gideon Lester'}\n",
    ")\n",
    "memory.save_context(\n",
    "    {'input': 'Who is Frederick Gideon Lester? Be brief no more than 20 words'},\n",
    "    {'output': \"Frederick Gideon Lester is a Hogwarts student in Harry's year.\"}\n",
    ")\n",
    "\n",
    "print_memory(memory, {\"prompt\": \"What is Frederick surename?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Agentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü¶ôü§ñüìú LLama Agent Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü¶ôü§ñü¶ú LLama Agent Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import AgentOutputParser\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from pydantic import PrivateAttr\n",
    "from typing import Union\n",
    "\n",
    "class CustomAgentParser(AgentOutputParser):\n",
    "    _chain: LLMChain = PrivateAttr(True)\n",
    "    \n",
    "    def __init__(self, expected_format: str, llm):\n",
    "        super().__init__()\n",
    "        \n",
    "        system_prompt = \"Your are a JSON expert. Your task is to correct a JSON document that is formated wrong. Act as a code generator, and always include in your response a version of JSON document corrected.\"\n",
    "        human_prompt = \"\"\"\n",
    "            Here is a piece of JSON code with some errors: \n",
    "            {error_json}\n",
    "            Please use your JSON kwonledge to fix it. The format that the document should follow is this, do not fix this format is only for reference:\n",
    "            {format}\n",
    "            \n",
    "            Fix the document in two steps:\n",
    "            Step 1: Indicate the errors in the JSON document. For example, missing closing quotes or arrays or documents. Be brief, no more than 20 words.\n",
    "            Step2: Build a JSON document with the errors fixed. Always return the document enclosed in the tags <json> and </json>\n",
    "        \"\"\"\n",
    "        prompt = llama_prompt(system_prompt, human_prompt,format=expected_format)\n",
    "        chain = {\"error_json\": RunnablePassthrough()} | prompt | llm | ExtractJsonOutputParser()\n",
    "        self._chain = chain\n",
    "\n",
    "    def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
    "        \n",
    "        final_answer = self._final_answer(text)\n",
    "        if not final_answer is None:\n",
    "            return AgentFinish(\n",
    "                return_values={\"output\": final_answer}, \n",
    "                log=text\n",
    "            )\n",
    "\n",
    "        action_str = self._action(text)\n",
    "        if not action_str is None:\n",
    "            action = self._parse_json(action_str)\n",
    "            input = {arg['keyword']: arg['value'] for arg in action['inputs']}\n",
    "            return AgentAction(\n",
    "                log=self._clean_observations_from_log(text),\n",
    "                tool=action['name'], \n",
    "                tool_input=input\n",
    "            )    \n",
    "        \n",
    "        raise ValueError(\"Wrong format, not an action or an answer\")\n",
    "\n",
    "    def _final_answer(self, text:str) -> Union[None,str]:\n",
    "        pattern = r\"#Final Answer:(.*?)$\"\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "        answer = match.group(1).strip() if match else None\n",
    "        return answer\n",
    "\n",
    "    def _action(self, text:str) -> Union[None,str]:\n",
    "        # Define the regular expression pattern\n",
    "        pattern = r\"#Action:(.*?)(?=#Observation:|$)\"\n",
    "        \n",
    "        # Use re.findall to extract the text between the tags\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        \n",
    "        # Clean up the extracted text (remove leading/trailing whitespace)\n",
    "        cleaned_matches = [match.strip() for match in matches]\n",
    "\n",
    "        return cleaned_matches[-1] if len(cleaned_matches) > 0 else None\n",
    "\n",
    "    def _parse_json(self, text: str):\n",
    "        json_regex = re.compile(r'<json>(.*)</json>', re.DOTALL)\n",
    "        \n",
    "        documents = json_regex.findall(text)\n",
    "        if len(documents) == 0:\n",
    "            return self._fix_json_errors(text)\n",
    "        \n",
    "        json_content = documents[0]\n",
    "        try:\n",
    "            return json.loads(json_content)\n",
    "        except json.JSONDecodeError:\n",
    "            return self._fix_json_errors(json_content)\n",
    "\n",
    "    def _clean_observations_from_log(self, text:str):\n",
    "        pattern = r\"(?s)(#Observation:[^\\n]*$)\"\n",
    "        text_without_observation = re.sub(pattern, \"\", text)\n",
    "        return text_without_observation\n",
    "\n",
    "    def _fix_json_errors(self, json_text: str):\n",
    "            return json.loads(self._chain.invoke(json_text))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü¶ôü§ñüí¨ LLama Agent Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import LLMSingleActionAgent\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "\n",
    "def format_scratchpad(steps):\n",
    "    scratchpad = \"\"\n",
    "    for action, value in steps:\n",
    "        scratchpad += f\"{action.log}\\n[INST]#Observation:{value}[/INST]\\n\"\n",
    "    return scratchpad\n",
    "\n",
    "def toolkit_prompt(toolkit):\n",
    "    tools_dict = {\n",
    "        \"tools\": [format_tool_to_openai_function(t) for t in toolkit]\n",
    "    }\n",
    "    # return a string with the json\n",
    "    return f\"<json>{json.dumps(tools_dict,indent=2)}</json>\"\n",
    "\n",
    "def create_llama_agent(toolkit):\n",
    "    system_prompt = \"\"\"\n",
    "    <<SYS>>\n",
    "    You are a helpful AI asistant capable of reasoning and using tools. Answer the question the best you can.\n",
    "    Explain every decission you take, be brief do it in as few words as possible, 50 maximum.\n",
    "    When neccesary use the tools indicated by the user to answer the question. \n",
    "    If there is no tool that helps you answering the question, try answering the question yourself or indicate that you do not know if you lack the information to answer it. \n",
    "    Only use the tools to do the actions they were designed for, never apply a tool to do a function they were not designed for.\n",
    "    You never know the result of using a tool. Never try to guess that result. The results of using a tool will be inputed by the user. \n",
    "    Only use one tool at a time. When using a tool, stop and wait for the user to input the \"#Observation:\" value. \n",
    "    You are not allowed to write \"#Observation:\", that keyword is reserved for the user.\n",
    "    Always start your intervition with a new thought using the keyword \"#Thought\".\n",
    "    <</SYS>>\n",
    "    \"\"\"\n",
    "    \n",
    "    human_prompt = \"\"\"\n",
    "    [INST] Your task is to answer the question posed by the user. When neccesary, use the next set of tools to answer the question, please use the descriptions to know when to select a tool. \n",
    "    Never use a tool that is not included in the document or for a purpose other than the one stated in its description:\n",
    "    {tools}\n",
    "    \n",
    "    To use a tool please follow the next JSON sintax:\n",
    "    {action_example}\n",
    "    \n",
    "    You must always follow the next steps to aswer a question. You may need to use several tools to answer the question or no tool at all. Never try to use other tools outside the ones provided. Never use a tool for things outside their description.  \n",
    "    In case no suitable tool is available for answering the question, skip the #Action and #Observation steps, go to the '#Final Answer' and try to answer the question yourself.\n",
    "    \n",
    "    #Question: the input question you must answer\n",
    "    #Thought: List the tools that could be use to answer the question and decide what tool to use next if any. Always explain why the tools are suitable and why not.\n",
    "    #Action: Only if there is suitable tool to answer the question, indicate which tool to use. Always finish the action with a JSON document enclosed between <json> and </json> tags. Wait for user input\n",
    "    <json>....</json>\n",
    "    #Observation: The user will input the result of executing the desired action. Never write the #Observation, wait for the user input.\n",
    "    #Thought: Analize the past chain of thoughts and the previous observation to decide what to do next. If more tools are necesary, do the next call. If no more tools are neccesary, answer the question.\n",
    "    #Action: If necessary, call the next tool to be used. Wait for user input\n",
    "    #Observation: \n",
    "    ... (repear N times)\n",
    "    #Action:\n",
    "    #Observation:\n",
    "    #Thought: Check that you have all the information neccesary to answer the question and no more tools are pending to be called. If so, you can answer the question.\n",
    "    #Final Answer: the final answer to the original input question. \n",
    "    \n",
    "    #### BEGIN THE TASK #####\n",
    "    \n",
    "    #Question: {input} [/INST]\n",
    "    {agent_scratchpad}\n",
    "    #Thought: \n",
    "    \"\"\"\n",
    "    \n",
    "    action_example = \"\"\"\n",
    "    <json>\n",
    "    {{\n",
    "        \"type\": \"action\",\n",
    "        \"name\": string, #one of the next values {tool_names}\n",
    "        \"inputs\": Array of {{\n",
    "            \"keyword\": string,\n",
    "            \"value\": Any\n",
    "        }}\n",
    "    }}\n",
    "    </json>\n",
    "    \"\"\"\n",
    "    action_example = action_example.format(tool_names=[t.name for t in toolkit])\n",
    "\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        template=system_prompt+\"\\n\"+human_prompt,\n",
    "        partial_variables={\n",
    "            \"tools\":toolkit_prompt(toolkit), \n",
    "            \"action_example\":action_example\n",
    "        }\n",
    "    )\n",
    "    parser = CustomAgentParser(\n",
    "        expected_format=action_example,\n",
    "        llm=llm\n",
    "    )        \n",
    "    \n",
    "    agent =  (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"input\"],\n",
    "            \"agent_scratchpad\": lambda x: format_scratchpad(x['intermediate_steps'])\n",
    "        } | prompt | llm | parser\n",
    "    )\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ‚öôÔ∏è Agent Running Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.agents import tool\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the length of a word.\n",
    "    \"\"\"\n",
    "    return len(word)\n",
    "\n",
    "@tool\n",
    "def pow(number:float, N:int) -> float:\n",
    "    \"\"\"\n",
    "    Returns the number to the power N\n",
    "    \"\"\"\n",
    "    return number**N\n",
    "\n",
    "toolkit = [get_word_length, pow]\n",
    "\n",
    "agent = create_llama_agent(toolkit)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=toolkit, verbose=True, max_iterations=6)\n",
    "\n",
    "agent_executor.invoke({\n",
    "    #\"input\": \"how many letters in the word 'educa'?\"\n",
    "    \"input\": \"Please translate the next sentence into emojis: 'I have cooked pig ribs for dinner'\"\n",
    "    #\"input\": \"What is 2 to the power of 3?\"\n",
    "    #\"input\": \"Count the number of letter in the word 'luna' and raise the result to the power of 3\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
